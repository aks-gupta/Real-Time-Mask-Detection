{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\akshi'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='C:\\\\Users\\\\akshi\\\\Documents\\\\Verzeo\\\\Mask Detection'\n",
    "categories=[\"with_mask\",\"without_mask\"]\n",
    "label_dict={\"with_mask\":0,\"without_mask\":1}\n",
    "labels=[0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "target=[]\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "        try: \n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "            resized=cv2.resize(gray,(100,100))\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376 1376\n"
     ]
    }
   ],
   "source": [
    "print(len(data) , len(target) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1376, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "data = data/255\n",
    "data=np.reshape(data,(data.shape[0],100,100,1))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1376, 2)\n"
     ]
    }
   ],
   "source": [
    "target = np.array(target)\n",
    "new_target=np_utils.to_categorical(target)\n",
    "print(new_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def grayscale(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "def equalize(img):\n",
    "    img =cv2.equalizeHist(img)\n",
    "    return img\n",
    "def preprocessing(img):\n",
    "    img = grayscale(img)\n",
    "    img = equalize(img)\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel() :\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16,(3,3),input_shape=data.shape[1:],activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50,activation=\"relu\"))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_target,test_target=train_test_split(data,new_target,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen= ImageDataGenerator(width_shift_range=0.2,   # 0.1 = 10%     IF MORE THAN 1 E.G 10 THEN IT REFFERS TO NO. OF  PIXELS EG 10 PIXELS\n",
    "                            height_shift_range=0.2,\n",
    "                            zoom_range=0.3,  # 0.2 MEANS CAN GO FROM 0.8 TO 1.2\n",
    "                            shear_range=0.1,  # MAGNITUDE OF SHEAR ANGLE\n",
    "                            rotation_range=15)  # DEGREES\n",
    "dataGen.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 47, 47, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                160050    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 174,200\n",
      "Trainable params: 174,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=createModel()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6615 - accuracy: 0.6068WARNING:tensorflow:From C:\\Users\\akshi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\akshi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "25/25 [==============================] - 4s 149ms/step - loss: 0.6611 - accuracy: 0.6078 - val_loss: 0.6590 - val_accuracy: 0.5389\n",
      "Epoch 2/30\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5843 - accuracy: 0.6940INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "25/25 [==============================] - 4s 146ms/step - loss: 0.5845 - accuracy: 0.6935 - val_loss: 0.5275 - val_accuracy: 0.8187\n",
      "Epoch 3/30\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.4684 - accuracy: 0.7760INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "25/25 [==============================] - 4s 151ms/step - loss: 0.4679 - accuracy: 0.7766 - val_loss: 0.3733 - val_accuracy: 0.8808\n",
      "Epoch 4/30\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.3216 - accuracy: 0.8737INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 0.3220 - accuracy: 0.8727 - val_loss: 0.3118 - val_accuracy: 0.8756\n",
      "Epoch 5/30\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.2346 - accuracy: 0.9167INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "25/25 [==============================] - 4s 148ms/step - loss: 0.2341 - accuracy: 0.9169 - val_loss: 0.2481 - val_accuracy: 0.8964\n",
      "Epoch 6/30\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.1624 - accuracy: 0.9375INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "25/25 [==============================] - 4s 152ms/step - loss: 0.1622 - accuracy: 0.9377 - val_loss: 0.1559 - val_accuracy: 0.9585\n",
      "Epoch 7/30\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.1318 - accuracy: 0.9466INFO:tensorflow:Assets written to: model-007.model\\assets\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 0.1315 - accuracy: 0.9468 - val_loss: 0.1257 - val_accuracy: 0.9689\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 4s 163ms/step - loss: 0.0861 - accuracy: 0.9766 - val_loss: 0.1461 - val_accuracy: 0.9430\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.1015 - accuracy: 0.9623 - val_loss: 0.1284 - val_accuracy: 0.9585\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0664 - accuracy: 0.9779 - val_loss: 0.1343 - val_accuracy: 0.9585\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 3s 137ms/step - loss: 0.0802 - accuracy: 0.9727 - val_loss: 0.1515 - val_accuracy: 0.9378\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 0.0390 - accuracy: 0.9870 - val_loss: 0.1603 - val_accuracy: 0.9534\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 0.1460 - val_accuracy: 0.9741\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 3s 137ms/step - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.1778 - val_accuracy: 0.9534\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 0.0169 - accuracy: 0.9987 - val_loss: 0.1621 - val_accuracy: 0.9689\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0252 - accuracy: 0.9909 - val_loss: 0.2909 - val_accuracy: 0.9275\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0123 - accuracy: 0.9987 - val_loss: 0.1753 - val_accuracy: 0.9585\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9637\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.1850 - val_accuracy: 0.9637\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.0093 - accuracy: 0.9961 - val_loss: 0.1906 - val_accuracy: 0.9585\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 0.0092 - accuracy: 0.9961 - val_loss: 0.2225 - val_accuracy: 0.9534\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 4s 145ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.2143 - val_accuracy: 0.9482\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 4s 160ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9430\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 4s 166ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2290 - val_accuracy: 0.9585\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 4s 167ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2782 - val_accuracy: 0.9534\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2260 - val_accuracy: 0.9534\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 4s 150ms/step - loss: 0.0232 - accuracy: 0.9922 - val_loss: 0.1995 - val_accuracy: 0.9637\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 4s 149ms/step - loss: 0.0130 - accuracy: 0.9974 - val_loss: 0.2012 - val_accuracy: 0.9585\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 4s 140ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2202 - val_accuracy: 0.9534\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 4s 142ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2288 - val_accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint=ModelCheckpoint(\"model-{epoch:03d}.model\",save_best_only=True,mode=\"auto\")\n",
    "history=model.fit(train_data,train_target,epochs=30,validation_split=0.2,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 35ms/step - loss: 0.1824 - accuracy: 0.9613\n",
      "[0.18237891793251038, 0.9612590670585632]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_data,test_target)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akshi\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\akshi\\assets\n"
     ]
    }
   ],
   "source": [
    "path_to_save= os.getcwd()\n",
    "print(path_to_save)\n",
    "model.save(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
